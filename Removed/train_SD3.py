'''
Author: Mingxin Zhang m.zhang@hapis.k.u-tokyo.ac.jp
Date: 2025-04-22 16:02:06
LastEditors: Mingxin Zhang
LastEditTime: 2025-04-23 02:45:41
Copyright (c) 2025 by Mingxin Zhang, All Rights Reserved. 
'''
import os
import math
import glob
import shutil
import subprocess
import argparse
import random
import pickle
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from tqdm.auto import tqdm

from torchvision import transforms

# Diffusers (Hugging Face)
from diffusers import (
    AutoencoderKL,
    DDPMScheduler,
    SD3Transformer2DModel,
    StableDiffusion3Pipeline,
    DiffusionPipeline
)
from diffusers.optimization import get_scheduler
from diffusers.training_utils import compute_snr

from huggingface_hub import hf_hub_download, whoami

from peft import LoraConfig, get_peft_model, PeftModel
import cv2

DEVICE = torch.device("cuda")


#################################################################################
#                             Dataset Part                          #
#################################################################################

def get_texture_folders(root_dir):
    return [os.path.join(root_dir, texture) for texture in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, texture))]

class PairedRandomFlip:
    def __init__(self, p=0.5):
        self.p = p

    def __call__(self, image, target):
        if random.random() < self.p:
            image = transforms.functional.hflip(image)
            target = transforms.functional.hflip(target)
        if random.random() < self.p:
            image = transforms.functional.vflip(image)
            target = transforms.functional.vflip(target)
        return image, target

class TextureDataset(Dataset):
    def __init__(self, root_dir, transform=None, paired_transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.paired_transform = paired_transform
        self.file_pairs = self._load_file_pairs()
    
    def _load_file_pairs(self):
        file_pairs = []
        texture_folders = get_texture_folders(self.root_dir)
        
        for texture_folder in texture_folders:
            texture_name = texture_folder.split("\\")[1]
            
            files = os.listdir(texture_folder)
            base_names = set(f.split(".")[0] for f in files)
            
            for base in base_names:
                image_path = os.path.join(texture_folder, f"{base}.jpg")
                heightmap_path = os.path.join(texture_folder, f"{base}.pkl")
                
                if os.path.exists(image_path) and os.path.exists(heightmap_path):
                    file_pairs.append((image_path, heightmap_path))
        
        return file_pairs
    
    def __len__(self):
        return len(self.file_pairs)
    
    def __getitem__(self, idx):
        image_path, heightmap_path = self.file_pairs[idx]
        image = Image.open(image_path).convert("RGB")
        with open(heightmap_path, 'rb') as f:
            heightmap = pickle.load(f).astype(np.float32)
        
        if self.transform:
            image = self.transform(image)
            heightmap = self.transform(heightmap)
            
            h_max = heightmap.max()
            h_min = heightmap.min()
            heightmap = (255 * (heightmap - h_min) / (h_max - h_min)).clamp(0, 255).byte()
            heightmap = heightmap / 255
        
        if self.paired_transform:
            image, heightmap = self.paired_transform(image, heightmap)
        
        return image, heightmap
    
#################################################################################
#                                       LoRA                                    #
#################################################################################  
def prepare_lora_model(lora_config, pretrained_model_name_or_path, model_path=None, weight_dtype=torch.bfloat16, resume=False, merge_lora=False):
    """
    (1) ç›®æ ‡:
        - åŠ è½½å®Œæ•´çš„ Stable Diffusion æ¨¡å‹ï¼ŒåŒ…æ‹¬ LoRA å±‚ï¼Œå¹¶æ ¹æ®éœ€è¦åˆå¹¶ LoRA æƒé‡ã€‚è¿™åŒ…æ‹¬ Tokenizerã€å™ªå£°è°ƒåº¦å™¨ã€UNetã€VAE å’Œæ–‡æœ¬ç¼–ç å™¨ã€‚

    (2) å‚æ•°:
        - lora_config: LoraConfig, LoRA çš„é…ç½®å¯¹è±¡
        - pretrained_model_name_or_path: str, Hugging Face ä¸Šçš„æ¨¡å‹åç§°æˆ–è·¯å¾„
        - model_path: str, é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
        - resume: bool, æ˜¯å¦ä»ä¸Šä¸€æ¬¡è®­ç»ƒä¸­æ¢å¤
        - merge_lora: bool, æ˜¯å¦åœ¨æ¨ç†æ—¶åˆå¹¶ LoRA æƒé‡

    (3) è¿”å›:
        - noise_scheduler: DDPMScheduler
        - transformer: SD3Transformer2DModel
        - vae: AutoencoderKL
    """
    # åŠ è½½å™ªå£°è°ƒåº¦å™¨ï¼Œç”¨äºæ§åˆ¶æ‰©æ•£æ¨¡å‹çš„å™ªå£°æ·»åŠ å’Œç§»é™¤è¿‡ç¨‹
    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, cache_dir=model_path, subfolder="scheduler")

    # åŠ è½½ VAE æ¨¡å‹ï¼Œç”¨äºåœ¨æ‰©æ•£æ¨¡å‹ä¸­å¤„ç†å›¾åƒçš„æ½œåœ¨è¡¨ç¤º
    vae = AutoencoderKL.from_pretrained(
        pretrained_model_name_or_path,
        cache_dir=model_path,
        torch_dtype=weight_dtype,
        subfolder="vae"
    )

    # åŠ è½½ UNet æ¨¡å‹ï¼Œè´Ÿè´£å¤„ç†æ‰©æ•£æ¨¡å‹ä¸­çš„å›¾åƒç”Ÿæˆå’Œæ¨ç†è¿‡ç¨‹
    transformer = SD3Transformer2DModel.from_pretrained(
        pretrained_model_name_or_path,
        cache_dir=model_path,
        torch_dtype=weight_dtype,
        subfolder="transformer"
    )
    
    # å¦‚æœè®¾ç½®ä¸ºç»§ç»­è®­ç»ƒï¼Œåˆ™åŠ è½½ä¸Šä¸€æ¬¡çš„æ¨¡å‹æƒé‡
    if resume:
        if model_path is None or not os.path.exists(model_path):
            raise ValueError("å½“ resume è®¾ç½®ä¸º True æ—¶ï¼Œå¿…é¡»æä¾›æœ‰æ•ˆçš„ model_path")
        # ä½¿ç”¨ PEFT çš„ from_pretrained æ–¹æ³•åŠ è½½ LoRA æ¨¡å‹
        transformer = PeftModel.from_pretrained(transformer, os.path.join(model_path, "transformer"))

        # ç¡®ä¿ transformer çš„å¯è®­ç»ƒå‚æ•°çš„ requires_grad ä¸º True
        for param in transformer.parameters():
            if param.requires_grad is False:
                param.requires_grad = True
                
        print(f"âœ… å·²ä» {model_path} æ¢å¤æ¨¡å‹æƒé‡")

    else:
        # å°† LoRA é…ç½®åº”ç”¨åˆ° text_encoder å’Œ unet
        transformer = get_peft_model(transformer, lora_config)

        # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡
        print("ğŸ“Š transformer å¯è®­ç»ƒå‚æ•°:")
        transformer.print_trainable_parameters()
    
    if merge_lora:
        # åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹ï¼Œä»…åœ¨æ¨ç†æ—¶è°ƒç”¨
        transformer = transformer.merge_and_unload()

        # åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼
        transformer.eval()

    # å†»ç»“ VAE å‚æ•°
    vae.requires_grad_(False)

    # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU ä¸Šå¹¶è®¾ç½®æƒé‡çš„æ•°æ®ç±»å‹
    transformer.to(DEVICE, dtype=weight_dtype)
    vae.to(DEVICE, dtype=weight_dtype)
    
    return noise_scheduler, transformer, vae
    

#################################################################################
#                                  Training Loop                                #
#################################################################################

def main(args):
    try:
        info = whoami()
        print(f"Authenticated to Huggingface Hub as: {info['name']}")
    except Exception as e:
        raise RuntimeError("You are not logged into Huggingface Hub. Please run 'huggingface-cli login' first.")
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    weight_dtype = torch.bfloat16  # æƒé‡æ•°æ®ç±»å‹ï¼Œä½¿ç”¨ bfloat16 ä»¥èŠ‚çœå†…å­˜å¹¶åŠ å¿«è®¡ç®—é€Ÿåº¦
    snr_gamma = 5  # SNR å‚æ•°ï¼Œç”¨äºä¿¡å™ªæ¯”åŠ æƒæŸå¤±çš„è°ƒèŠ‚ç³»æ•°
        
    # Stable Diffusion LoRA çš„å¾®è°ƒå‚æ•°

    # å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
    lr_scheduler_name = "cosine_with_restarts"  # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ä¸º Cosine annealing with restartsï¼Œé€æ¸å‡å°‘å­¦ä¹ ç‡å¹¶å®šæœŸé‡å¯
    lr_warmup_steps = 100  # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼Œåœ¨æœ€åˆçš„ 100 æ­¥ä¸­é€æ¸å¢åŠ å­¦ä¹ ç‡åˆ°æœ€å¤§å€¼
    max_train_steps = 2000  # æ€»è®­ç»ƒæ­¥æ•°ï¼Œå†³å®šäº†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹çš„è¿­ä»£æ¬¡æ•°
    num_cycles = 3  # Cosine è°ƒåº¦å™¨çš„å‘¨æœŸæ•°é‡ï¼Œåœ¨è®­ç»ƒæœŸé—´ä¼šé‡å¤ 3 æ¬¡å­¦ä¹ ç‡å‘¨æœŸæ€§é€’å‡å¹¶é‡å¯

    # LoRA é…ç½®
    lora_config = LoraConfig(
        r=32,  # LoRA çš„ç§©ï¼Œå³ä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œå†³å®šäº†å‚æ•°è°ƒæ•´çš„è‡ªç”±åº¦
        lora_alpha=16,  # ç¼©æ”¾ç³»æ•°ï¼Œæ§åˆ¶ LoRA æƒé‡å¯¹æ¨¡å‹çš„å½±å“
        target_modules=[
            "attn.add_k_proj", 
            "attn.add_q_proj", 
            "attn.add_v_proj", 
            "attn.to_add_out", 
            "attn.to_k", 
            "attn.to_out.0", 
            "attn.to_q", 
            "attn.to_v"
        ],
        lora_dropout=0  # LoRA dropout æ¦‚ç‡ï¼Œ0 è¡¨ç¤ºä¸ä½¿ç”¨ dropout
    )
    
    # Setup data:
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Grayscale(num_output_channels=1)
    ])
    paired_transform = PairedRandomFlip(p=0.5)
    root_dir = "../../Texture"
    dataset = TextureDataset(root_dir, transform=transform, paired_transform=paired_transform)
    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)

    print(f"Dataset size: {len(dataset)} samples")
    print(f"Starting training for {args.epochs} epochs...")
    
    # å‡†å¤‡æ¨¡å‹
    noise_scheduler, transformer, vae = prepare_lora_model(
        lora_config,
        "stabilityai/stable-diffusion-3.5-large",
        model_path=args.model_path,
        weight_dtype=weight_dtype,
        resume=False,
        merge_lora=False
    )

    transformer_lora_layers = [p for p in transformer.parameters() if p.requires_grad]
    
    # ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(transformer_lora_layers, lr=5e-4)

    # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = get_scheduler(
        lr_scheduler_name,
        optimizer=optimizer,
        num_warmup_steps=lr_warmup_steps,
        num_training_steps=max_train_steps,
        num_cycles=num_cycles
    )

    print("âœ… æ¨¡å‹å’Œä¼˜åŒ–å™¨å‡†å¤‡å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
    
    # ç¦ç”¨å¹¶è¡ŒåŒ–ï¼Œé¿å…è­¦å‘Š
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # åˆå§‹åŒ–
    global_step = 0

    # è¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
    progress_bar = tqdm(
        range(max_train_steps),  # æ ¹æ® num_training_steps è®¾ç½®
        desc="è®­ç»ƒæ­¥éª¤",
    )

    # è®­ç»ƒå¾ªç¯
    for epoch in range(math.ceil(max_train_steps / len(loader))):
        # å¦‚æœä½ æƒ³åœ¨è®­ç»ƒä¸­å¢åŠ è¯„ä¼°ï¼Œé‚£åœ¨å¾ªç¯ä¸­å¢åŠ  train() æ˜¯æœ‰å¿…è¦çš„
        transformer.train()
        
        for i, (input_images, target_images) in enumerate(loader):
            if global_step >= max_train_steps:
                break
            
            input_images = input_images.to(DEVICE, dtype=weight_dtype)
            target_images = target_images.to(DEVICE, dtype=weight_dtype)

            # 2. ç¼–ç æˆlatent
            with torch.no_grad():
                input_latents = vae.encode(input_images.repeat(1,3,1,1)).latent_dist.sample() * vae.config.scaling_factor
                target_latents = vae.encode(target_images.repeat(1,3,1,1)).latent_dist.sample() * vae.config.scaling_factor

            # ä¸ºæ½œåœ¨è¡¨ç¤ºæ·»åŠ å™ªå£°ï¼Œç”Ÿæˆå¸¦å™ªå£°çš„å›¾åƒ
            noise = torch.randn_like(target_latents)  # ç”Ÿæˆä¸æ½œåœ¨è¡¨ç¤ºç›¸åŒå½¢çŠ¶çš„éšæœºå™ªå£°
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (target_latents.shape[0],), device=DEVICE).long()
            noisy_latents = noise_scheduler.add_noise(target_latents, noise, timesteps)

            # è®¡ç®—ç›®æ ‡å€¼
            if noise_scheduler.config.prediction_type == "epsilon":
                target = noise  # é¢„æµ‹å™ªå£°
            elif noise_scheduler.config.prediction_type == "v_prediction":
                target = noise_scheduler.get_velocity(target_latents, noise, timesteps)  # é¢„æµ‹é€Ÿåº¦å‘é‡

            # transformer æ¨¡å‹é¢„æµ‹
            model_pred = transformer(hidden_states=noisy_latents, 
                                     encoder_hidden_states=input_latents, 
                                     pooled_projections=torch.zeros(args.batch_size, 2048, device=DEVICE, dtype=weight_dtype),    # No text encoder, a dummy input 
                                     timestep=timesteps)[0]

            # è®¡ç®—æŸå¤±
            if not snr_gamma:
                loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
            else:
                # è®¡ç®—ä¿¡å™ªæ¯” (SNR) å¹¶æ ¹æ® SNR åŠ æƒ MSE æŸå¤±
                snr = compute_snr(noise_scheduler, timesteps)
                mse_loss_weights = torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0]
                if noise_scheduler.config.prediction_type == "epsilon":
                    mse_loss_weights = mse_loss_weights / snr
                elif noise_scheduler.config.prediction_type == "v_prediction":
                    mse_loss_weights = mse_loss_weights / (snr + 1)
                
                # è®¡ç®—åŠ æƒçš„ MSE æŸå¤±
                loss = F.mse_loss(model_pred.float(), target.float(), reduction="none")
                loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
                loss = loss.mean()

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)
            global_step += 1

            # æ‰“å°è®­ç»ƒæŸå¤±
            if global_step % 100 == 0 or global_step == max_train_steps:
                print(f"ğŸ”¥ æ­¥éª¤ {global_step}, æŸå¤±: {loss.item()}")

            # ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹ï¼Œå½“å‰ç®€å•è®¾ç½®ä¸ºæ¯ 500 æ­¥ä¿å­˜ä¸€æ¬¡
            if global_step % 500 == 0:
                save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                os.makedirs(save_path, exist_ok=True)

                # ä½¿ç”¨ save_pretrained ä¿å­˜ PeftModel
                transformer.save_pretrained(os.path.join(save_path, "unet"))
                print(f"ğŸ’¾ å·²ä¿å­˜ä¸­é—´æ¨¡å‹åˆ° {save_path}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° checkpoint-last
    save_path = os.path.join(args.output_dir, "checkpoint-last")
    os.makedirs(save_path, exist_ok=True)
    transformer.save_pretrained(os.path.join(save_path, "unet"))
    print(f"ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {save_path}")

    print("ğŸ‰ å¾®è°ƒå®Œæˆï¼")
    

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", type=str, default="pretrained_models", help="Path to pre-trained SD3.5 model.")
    parser.add_argument("--output-dir", type=str, default="contents", help="Where to save lora adapters.")
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch-size", type=int, default=4)
    parser.add_argument("--num-workers", type=int, default=4)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--lora-rank", type=int, default=16)
    parser.add_argument("--lora-alpha", type=int, default=32)
    parser.add_argument("--lora-dropout", type=float, default=0.1)
    parser.add_argument("--log-every", type=int, default=50)
    args = parser.parse_args()

    main(args)