# HapticMatch: An Exploration for Generative Material Haptic Simulation and Interaction

This repository contains the dataset introduced in the paper: [HapticMatch: An Exploration for Generative Material Haptic Simulation and Interaction](https://arxiv.org/abs/2601.16639).

![Dataset](https://github.com/MXMX0811/GelSight_Texture_Dataset/blob/master/src/data.png?raw=true)

## Abstract

High-fidelity haptic feedback is essential for immersive virtual environments, yet authoring realistic tactile textures remains a significant bottleneck for designers. We introduce HapticMatch, a visual-to-tactile generation framework designed to democratize haptic content creation. We present a novel dataset containing precisely aligned pairs of micro-scale optical images, surface height maps, and friction-induced vibrations for 100 diverse materials. Leveraging this data, we explore and demonstrate that conditional generative models like diffusion and flow-matching can synthesize high-fidelity, renderable surface geometries directly from standard RGB photos. By enabling a "Scan-to-Touch" workflow, HapticMatch allows interaction designers to rapidly prototype multimodal surface sensations without specialized recording equipment, bridging the gap between visual and tactile immersion in VR/AR interfaces.

## Citation

If you find this repo is helpful, please cite:

```bibtex
@misc{zhang2026hapticmatchexplorationgenerativematerial,
      title={HapticMatch: An Exploration for Generative Material Haptic Simulation and Interaction}, 
      author={Mingxin Zhang and Yu Yao and Yasutoshi Makino and Hiroyuki Shinoda and Masashi Sugiyama},
      year={2026},
      eprint={2601.16639},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2601.16639}, 
}
```
